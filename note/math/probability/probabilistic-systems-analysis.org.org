#+title: probabilistic systems analysis

* links

  https://www.youtube.com/playlist?list=PLUl4u3cNGP61MdtwGTqZA0MreSaDybji8

* intro

  - probability as a mathematical framework
    for reasoning about uncertainty

* probabilistic models

  - sample space -- set of outcomes
    we record which outcome is more likely to occur compare to others
    by assigning probability to outcome.

    but to deal with continuous sample space
    we must adjust our method by assigning probability to events
    where an event is a subset of sample space

    this definition also handles discrete sample space well
    by viewing assigning probability to sets of one element

  - probability axioms

    - nonnegative -
      -- A : sample-space -> P (A) >= 0

    - normalization -
      P (sample-space) = 1

    - notations about set :
      | ^  | set intersection |
      | +  | set union        |
      | =< | sub set          |
      | >= | super set        |

    - additivity -
      -- A ^ B = empty-set
      -> P (A + B) = P (A) + P (B)

    - countable additivity -
      the additivity axiom need to be generalized to sequence of subsets
      instead of just two subsets
      (where sequence implies countable)

    the axioms looks like integration of some function over sample-space

  - extra notes about subsets of sample-space
    ignore ugly and weird subsets
    because for some weird subsets we can not both
    assigning probability to them and maintain probability axioms
    - x -
      this also revealed set theory's inconvenience,
      but why we are not using type theory yet ?
      will type theory be even more inconvenient ?

  - x -
    dealing with subsets remembers me of relational programming

  - discrete uniform distribution
    all outcomes be equally likely
    compute probability = counting

  - continuous uniform distribution
    probability = area

* zero probability

  - if we define probability by volume in n dimension unit space
    zero probability does not mean impossible
    zero probability only means n dimensional volume is zero

* conditional probability

  - the story goes like this,
    you know something about this uncertain world,
    and based on what you know, you build a probability model,
    by write down probabilities for different outcomes.
    then something happened, you get some new informations,
    you know more about the world,
    these new informations should change your beliefs
    about what may happen and what may not happen.

  - partial informations about random experiments and revise beliefs

  - P (A | B) := the probability of A, given that B occurred
    P (A | B) := P (A ^ B) / P (B)
    - given P (B) != 0
    - if P (B) = 0, P (A | B) is undefined

  - P (A ^ B) = P (B) * P (A | B)
    P (A ^ B) = P (A) * P (B | A)

  - with the above definition
    the proportion of probabilities in B is maintained
    suppose A1 =< B and A2 =< B
    P (A1 | B) / P (A2 | B) =
    P (A1 ^ B) / P (A2 ^ B) =
    P (A1) / P (A2)

  - we specify the probability model by a conditional probability tree
    instead of calculate conditional probability of a given model

  - P (A ^ B) = P (A) * P (B | A)

    P (A ^ B ^ C) =
    P ((A ^ B) ^ C) =
    P (A ^ B) * P ((A ^ B) | C) =
    P (A) * P (B | A) * P ((A ^ B) | C)

  - P (B) =
    P (B ^ (A + ~A)) =
    P (B ^ A) + P (B ^ ~A) =
    P (A) * P (B | A) + P (~A) * P (B | ~A)

    P (B) =
    P (B ^ (A1 + A2 + A3)) =
    P (B ^ A1) + P (B ^ A2) + P (B ^ A3) =
    P (A1) * P (B | A1) +
    P (A2) * P (B | A2) +
    P (A3) * P (B | A3)

  - P (A | B) = P (A ^ B) / P (B)
    can be interpreted as an inference problem
    suppose B, what is the probability of A

    - where P (A ^ B) and P (B) can be calculated
      by the above two sections
      which goes from P (B | Ai) to P (Ai | B)

    - Bayes rule :
      we know
      Ai -> B -- P (B | Ai)
      we observe B, and we infer
      B -> Ai -- P (Ai | B)

* independence

  - ><
