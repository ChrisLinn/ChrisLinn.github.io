#+HTML_HEAD: <link rel="stylesheet" href="../asset/css/page.css" type="text/css" media="screen" />
#+title: function compose, type cut, and the algebra of logic

------
@@html:
<p> this paper is not finished yet </p>
@@
------
@@html:
<p> abstract : </p>
<p> </p>
@@
------
@@html:
<p> keywords : sequent calculus, deduction system, dependent type </p>
@@
------

* intuitionistic logic

  - first I would like to summarize intuitionistic logic
    it is also called constructive logic
    it is very well known now
    and is gloriously called the Brouwer–Heyting–Kolmogorov interpretation

  - to prove (P and Q)
    is to prove P and prove Q
    - this is the same as classical logic

  - to prove (P or Q)
    is to prove P or prove Q
    - while in classical logic
      you can prove (P or Q)
      without a proof of P
      and without a proof of Q

  - to prove (P -> Q)
    is to prove that
    if we have a proof of P
    then we can construct a proof of Q
    - this is the same as classical logic

  - to prove (not P)
    is to prove (P -> something-we-consider-wrong)
    - something-we-consider-wrong like (0 = 1)
    - (not (not P)) is weaker than P
      while in classical logic
      (not (not P)) is equal to P

  - to prove "for all x belong to A, we have P(x)"
    is to prove that
    for all x belong to A
    we can construct a proof of P(x)
    - how to construct a proof discuss later
    - this is the same as classical logic

  - to prove "there exist x belong to A, such that P(x)"
    is to construct a value of type A
    and construct a proof of P(x)
    - the only way to prove something exist
      is to find such thing
    - while in classical logic
      to prove something exist
      you do not need to find such thing

* formal theorem

  - let us design a formal language to express theorem
    - [[remark on formalization]]

  - firstly we see the fact that the general form of theorem is like (A -> B)
    let us unite our syntax toward "->"
    we do not write A
    instead we will write (-> A)
    - this is just like one does not write 3 but write 1\3 or 3/1 instead

  - and let us optimize our syntax for "and"
    we do not write ((A and B) -> (C and D))
    but just write (A B -> C D)

  - I call express of form (A B C ... -> E F G ...) sequent or arrow
    this term is taken from Gentzen
    but you should note that
    sequent for us is not exactly the same as sequent for Gentzen
    for Gentzen (A B -> C D) is viewed as ((A and B) -> (C or D))
    but for us (A B -> C D) is viewed as ((A and B) -> (C and D))
    - if you want to know more about the meaning of sequent for Gentzen
      please see his paper "investigations into logical deduction"

  - and let us write (or A B C) instead of (A or B or C)

  - I suggest to ignore "not"
    because you see that (not P) is just (P -> something-we-consider-wrong)
    the negation we want to express
    is parameterized by something-we-consider-wrong
    (or say, depends on something-we-consider-wrong)
    if we simple say (not P)
    the information of this parameterization will be lost

  - I express "for all x belong to A, we have P(x)"
    as ((x : A) -> x P)
    and express "there exist x belong to A, such that P(x)"
    as (-> (x : A) x P)
    I am using postfix notation here
    I write "x P" instead of "P(x)"
    - you might argue that
      compare to the traditional math notation, postfix looks really alien
      if you care, please see my [[rationale of using postfix notation]]

  - recall that
    ((x : A) -> x P) means "for all x belong to A, we have P(x)"
    (-> (x : A) x P) means "there exist x belong to A, such that P(x)"
    in the above two example expressions
    variables are written in lower-case latin letter
    functions are written in upper-case latin letter
    personally I do not wish to
    distinguish meaning by lower-case v.s. upper-case
    (maybe because I am a scheme user, or maybe because I am a Chinese)
    so, in the following implementation of our language
    I will write
    #+begin_src scheme
    ((:x : a) -> :x p)
    (-> (:x : a) :x p)
    #+end_src

  - now we have designed a formal language to express theorem
    the best way to show how to use formal theorem in deduction
    is a detour through functional programming language
    theorem -> type -> function -> proof

* theorem as type

  - theorem can be viewed as type of function
    this is well known as "theorem as type"

  - that means the language we just designed for formal theorem
    can also be used to describe the type of function

  - our functional programming language will be a term-rewriting-system
    with postfix notation

  - examples about natural number
    #+begin_src scheme
    (+ natural (-> type)
       zero (-> natural)
       succ (natural -> natural))

    (~ add (natural natural -> natural)
       (:m zero -> :m)
       (:m :n succ -> :m :n add succ))
    #+end_src

  - in the above example
    "+" can be read as "define-type"
    the type is "natural" with type (-> type)
    and its two data-constructors
    are "zero" with type (-> natural)
    and "succ" with type (natural -> natural)
    - each of them has a type
      means each of them can be viewed as a function
      although they have no function body
      because when viewed as function, they are trivial
    - they are actually not only trivial
      but also reversible
      this means they can be used as pattern in pattern-matching
      because every reversible function
      can be used as pattern in pattern-matching
      (although I am not able to achieve it in this language)

  - in the above example
    "~" can be read as "define-function"
    the function is "add" with type (natural natural -> natural)
    and its function body has two arrows
    first (:m zero -> :m)
    second (:m :n succ -> :m :n add succ)
    - I will call the left part of an arrow "antecedent"
      and the right part of an arrow "succedent"
      so we have
      ( "antecedent" -> "succedent" )

  - the semantic of function
    can be explained by explaining
    what happens when we apply a function to its arguments

  - when applying a function
    the interperter will try to match (or cover) its arguments
    with the antecedent of each arrow in the function body
    - for "add", the antecedents will be (:m zero) and (:m :n zero)
    when one antecedent successes
    it will bind variables occurs in the antecedent to data in arguments
    and rewrite its corresponding succedent
    and returned the result
    - because all functions are "cover-checked"
      there must be at least one antecedent can cover the arguments
      then there are many antecedents can cover the arguments
      the first one is used
    - note that
      the order of rewriting is revealed by the postfix notation
    - "use the first covering antecedent" and "the order of rewriting"
      conclude the reduction strategy of our term-rewriting-system

  - because we are using postfix notation
    beside function application
    we can also use function composition to explain the semantic of function
    with the help of a stack
    we will be able to handle function composition of functions
    with multiple arguments and multiple return values
    for example, we can define the following stack processing functions
    #+begin_src scheme
    (~ drop (:t ->)
       (:d ->))

    (~ dup (:t -> :t :t)
       (:d -> :d :d))

    (~ over (:t1 :t2 -> :t1 :t2 :t1)
       (:d1 :d2 -> :d1 :d2 :d1))

    (~ tuck (:t1 :t2 -> :t2 :t1 :t2)
       (:d1 :d2 -> :d2 :d1 :d2))

    (~ swap (:t1 :t2 -> :t2 :t1)
       (:d1 :d2 -> :d2 :d1))
    #+end_src
    for more about this
    please read [[remark on the use of stack in implementation]]

  - more examples about natural number
    #+begin_src scheme
    (~ mul (natural natural -> natural)
       (:m zero -> zero)
       (:m :n succ -> :m :n mul :m add))

    (~ factorial (natural -> natural)
       (zero -> zero succ)
       (:n succ -> :n factorial :n succ mul))
    #+end_src

  - examples about list
    #+begin_src scheme
    (+ list ({:t : type} :t -> type)
       null (-> :t list)
       cons (:t list :t -> :t list))

    (~ append (:t list :t list -> :t list)
       (:l null -> :l)
       (:l :r :e cons -> :l :r append :e cons))

    (~ map (:t1 list (:t1 -> :t2) -> :t2 list)
       (null :f -> null)
       (:l :e cons :f -> :l :f map :e :f apply cons))
    #+end_src

  - examples about vector
    function bodys are the same as examples about list
    but the types also express the information about the length of list
    #+begin_src scheme
    (+ vector ({:t : type} natural :t -> type)
       null (-> zero :t vector)
       cons (:n :t vector :t -> :n succ :t vector))

    (~ append (:m :t vector :n :t vector -> :m :n add :t vector)
       (:l null -> :l)
       (:l :r :e cons -> :l :r append :e cons))

    (~ map (:n :t1 vector (:t1 -> :t2) -> :n :t2 vector)
       (null :f -> null)
       (:l :e cons :f -> :l :f map :e :f apply cons))
    #+end_src

  - examples that use lambda
    #+begin_src scheme
    ><><><
    #+end_src

* function as proof

  - now we are at the finial step of the detour from formal theorem to proof
    theorem -> type -> function -> proof

  - this is well known as "function as proof"
    it says, the way we write functions forms a language to record deduction
    a record of many steps of deduction is called a proof

  - I agree with Wittgenstein
    that a rule cannot be used to explain an action
    because any course of action can be made out to accord with some rule

  - so, first we observe actions
    then we design some rules to summarize them
    (or compare what we observed with existing deduction rules (or inference rules))

  - the question is
    when we use this kind of syntax to write function body
    what actions upon types we are recording ?
    (what deductions upon theorems we are recording ?)

*** concatenation, composition and cut

    - first syntax operation is concatenation
      concatenation of two names corresponds to
      1. composition of two functions under these names
      2. cut of two types under these names

    - [[rationale of composition over application]]

    - by "cut" I mean the hero deduction rule
      which occupys the center of the stage of Gentzen's sequent calculus
      it says if we have (A -> B) and (B -> C)
      cut them together, we get (A -> C)

    - on the other hand
      if we have function f1 of type (A -> B) and f2 of (B -> C)
      compose f1 and f2, we get a function of type (A -> C)
      this is what I mean by "function compose, type cut" in the title

    - in the following example
      "*" can be read as "define-hypothesis"
      #+begin_src scheme
      (* wanderer/poe (-> poe is-wanderer))
      (* way-worn (:x is-wanderer -> :x is-weary))

      (~ weary/poe (-> poe is-weary)
         (-> wanderer/poe way-worn))
      #+end_src

    - when view them as functions and types
      it is really intuitive to see
      with two functions "wanderer/poe" and "way-worn"
      how we can compose a function of type (-> poe is-weary)
      this is why I said that
      the best way to show how to use formal theorems in deduction
      is a detour through functional programming language

*** other deduction rules of natural deduction

    - the following seems like conj-intro and conj-elim in natural deduction
      we can simply use stack processing function to express them
      - the types of stack processing functions
        should remind you of the so called structural rules of sequent calculus
      #+begin_src scheme
      ;; conj-intro
      (* p1 (-> a))
      (* p2 (-> b))
      (~ p3 (-> a b)
         (-> p1 p2))

      (* drop (:t ->)
         (:d ->))
      (~ swap (:t1 :t2 -> :t2 :t1)
         (:d1 :d2 -> :d2 :d1))

      ;; conj-elim
      (* p3 (-> a b))
      (~ p1 (-> a)
         (-> p3 drop))
      (~ p2 (-> b)
         (-> p3 swap drop))
      #+end_src

*** the meaning of proof

    - we have the advantage to observe
      the concrete meaning of "proof" within our concrete model

    - concretely, how proof (type) is checked by the language ?
      I have the following summarization

      | arrow list in function body |             |
      |-----------------------------+-------------|
      | for each arrow              | type-check  |
      | for all antecedents         | cover-check |
      | for each succedent          | recur-check |

    - to type-check one arrow, is to
      - unify the antecedent of type-arrow
        with the type of the antecedent of arrow
      - during which, variables will be bound to data or other variables
      - under these bindings
        try to cover the succedent of type-arrow
        by the type of the succedent of arrow

    - for example, if we define natural number as the following
      then we can proof natural-induction
      #+begin_src scheme
      (+ natural (-> type)
         zero (-> natural)
         succ (natural -> natural))

      (~ natural-induction ((:p : (natural -> type))
                            zero :p apply
                            ((:k : natural) :k :p apply -> :k succ :p apply)
                            (:x : natural) -> :x :p apply)
         (:q :q/z :q/s zero -> :q/z)
         (:q :q/z :q/s :n succ ->
             :n
             :q :q/z :q/s :n natural-induction
             :q/s apply))

      ;; take the type check of the second arrow for example

      ;; unify the antecedent of type-arrow :
      ((:p : (natural -> type))
       zero :p apply
       ((:k : natural) :k :p apply -> :k succ :p apply)
       (:x : natural))

      ;; with the type of antecedent of the second arrow :
      type of (:q :q/z :q/s :n succ)

      ;; bindings :
      ((:p = :q)
       (:q : (natural -> type))
       (:q/z : zero :p apply)
       (:q/s : ((:k : natural) :k :p apply -> :k succ :p apply))
       (:x = :n)
       (:n : natural))

      ;; the type of the succedent of the second arrow :
      type of
      (:n
       :q :q/z :q/s :n natural-induction
       :q/s apply)
      == ;; under bindings
      ((:n : natural)
       (:q : (natural -> type))
       (:q/z : zero :q apply)
       (:q/s : ((:k : natural) :k :q apply -> :k succ :q apply))
       (:n : natural)
       natural-induction
       :q/s type/apply)
      ==
      ((:n : natural)
       :n :q apply
       :q/s type/apply)
      ==
      ((:n succ :q apply))

      ;; cover the succedent of type-arrow :
      (:x :p apply)
      == ;; under bindings
      ((:n succ :q apply))
      #+end_src

    - to summarize the meaning of "proof" within our concrete model
      - we can express theorems about
        - recursively defined data
        - recursively defined function
      - we can do prove by
        - cut -- function composition
        - exhaustion -- cover-check
        - structural induction --
          where first we proof some basic steps
          and by unification we change get next-theorem
          (just as the next-number in natural-induction)
          a function recursive call is a use of the induction hypothesis

*** >< the meaning of type definition

    define new type
    exist-intro

    branching by a list of arrow
    exist-elim

    binding by unification
    conj-elim

* >< algebra of logic

  - carefully define equality of theorem, we will get a natural field

    | deduction   | language to record deduction | logic field     |
    |-------------+------------------------------+-----------------|
    | cut         | function composition         | weaken          |
    | exist-intro | define new type              | field extension |
    | exist-elim  | branching by a list of arrow | distributive    |
    | conj-elim   | binding by unification       |                 |

  - ><><><

*** the natural field

    - let us view theorem (A -> B) as fraction
      A as denominator
      B as numerator
      - so, one might write (A \ B)
        note that
        we are using reverse-slash instead of slash
        to maintain the order of A B in (A -> B)

    - theorems under addition is an Abelian semigroup
      we do not have identity element
      and we do not have inverse
      - of course, we can introduce a "zero-theorem"
        (a theorem that we can never prove)
        as the identity element of addition
        to make our algebraic structure more like fraction of natural number
        but let us do not do this for now

    - to add two theorems (A -> B) and (C -> D)
      we get (A B -> (B C or A D))
      - just like (A \ B) + (C \ D) = (A C \ (B C + A D))

    - to multiply two theorems (A -> B) and (C -> D)
      we get (A C -> B D)
      - just like (A \ B) (C \ D) = (A C \ B D)

    - theorems under multiplication is an Abelian group
      identity element is (->)
      inverse of (A -> B) is (B -> A)

    - distributive is just like fraction of natural number
      because the way we define addition
      is just like the addition of fraction of natural number

    - I would like to coin a new term "natural field"
      for our algebraic structure
      to recall its similarites between the fraction of natural number
      - note that
        other terms like 'semi-field' is ambiguous
        because it does not inform us
        whether addition or multiplication is semi

*** the order structure of our natural field

    - the next question one should ask is
      what is the relation between this natural field and deduction ?
      the answer relates to the order structure of our natural field
      (actually we have a lattice, I will address its detail in another article)

    - just like natural number
      we have an order between elements of natural field
      I will use the term "weaker" to denote this order relation
      for natural number, we say, x is less than y
      for natural field, let us say, x is weaker than y
      - but our definition will not be total
        thus we will only have a poset (partially ordered set)

    - let us define "weaker" as
      - (-> A) is weaker than (-> (A or B))
        (-> B) is weaker than (-> (A or B))
      - (-> :x :x) is weaker than (-> :x :y)
        (-> :x P :x P) is weaker than (-> :x P :y P)
      - if X is weaker than Y
        then the reverse of Y is weaker than X

*** the relation between natural field of logic and deduction

    - now we can observe that
      deduction is
      to build new theorem by addition or multiplication theorems
      or weaken a theorem

    - cut can be viewed as an important way to weaken a theorem
      recall that
      if we have (A -> B) and (B -> C)
      cut them, we get (A -> C)
      multiply them, we get (A B -> B C)
      we can view cut as changing (A B -> B C) to (A -> C)
      - just like the fraction of natural number
        where (A B \ B C) = (A \ C)

    - I said that, cut can be viewed as weaken
      but the above example is not weakening the theorem at all
      while the following example do
      if we have theorem (A -> B) and ((B or D) -> C)
      cut them, we can deduce theorem (A -> C)
      - just like for the fraction of natural number
        we have (A B \ (D + B) C) > (A \ C)

*** to summarize

    - the algebraic structure of logic is a natural field
    - deduction is
      to build new theorem by addition and multiplication theorems
      or weaken a theorem
    - cut can be viewed as an important way to weaken a theorem
    - a proof is a record of many steps of deductions

* implementation

  - an attempt to implement such a language

  - project page : http://xieyuheng.github.io/sequent1

* appendix

*** remark on formalization

    - I agree with Errett Bishop who said
      "a proof is any completely convincing argument."
      I also think theorems expressed by formal language are specially clear
      and proofs checked by computer are specially convincing

    - on the other hand
      I also think that
      formal language can never be used to satisfactorily explain
      or totally simulate human language
      formal theorem and formal proof can never fully capture "human proof"
      this fact is specially clear
      if you are willing to think of "human proof" historically

    - the aim (or one aim) of formalization is to reduce (or remove) vagueness
      while the definition of vagueness is always vague

*** rationale of using postfix notation

    - rationale of using postfix notation is the following
      in the linear writing system of our language
      we can roughly distinguish four kinds of notations for function or predicate
      | infix     | ((1 + 2) + 3) |
      | prefix    | + + 1 2 3     |
      | postfix   | 3 2 1 + +     |
      | borderfix | (+ 1 2 3)     |
      - infix is especially good for associative binary function
      - prefix and postfix are not ambiguous without bracket
      - borderfix can be used for functions
        that can apply to different numbers of arguments
      our choice is between prefix and postfix
      because for simplicity we have the following two features
      - the arity of all functions must be fixed
      - we want our expressions to be not ambiguous without bracket
      then, how do we decide to use postfix instead of prefix ?
      seemingly, prefix and postfix are symmetric
      while we still can distinguish them
      because we write in special order (from left to right in most western language)
      in postfix notation suppose we have written
      1 2 +
      and we want to add 3 to the result of 1 2 +
      we simply write
      1 2 + 3 +
      while in prefix notation suppose we have written
      @@html: + 1 2 @@
      and we want to add 3 to the result of + 1 2
      we have to insert + 3 in front of + 1 2 and write
      @@html: + 3 + 1 2 @@
      I summarize this difference by say
      postfix notation respect the special order of a linear writing system
      the above conclude my rationale

*** >< remark on the use of stack in implementation

*** rationale of composition over application

    - to optimize system for composition
      is to denote composition by concatenation of term

    - when optimize syntax for composition instead of application
      - we get better algebra-like structure
        because function composition is associative
        while function application is not
      - we lost good syntax about currying
        because currying is designed as a convention
        of the syntax of function application

*** remark on deduction and inference

    - one might ask, what is a deduction or a inference ?
      my answer is a deduction or a inference
      is a way to express a change of theorem
      "a change" means "one step of change"

    - let us generalized a little bit
      and to discuss "a change of thing" and "language to record changes"
      you will find these two concepts are very common
      and they also are named gloriously in different places
      | thing   | a change of thing     | language to record changes |
      |---------+-----------------------+----------------------------|
      | theorem | deduction             | proof                      |
      | food    |                       | cookbook                   |
      | data    |                       | algorithm                  |
      | number  | elementary arithmetic |                            |
      (seems to me a market for language designer)

*** ><

    - how about (C or D) ? you might ask
      I would say, let us ignore (C or D) for now
      we will not be able to express such thing in our language
      but no worry
      because we will be able to express
      "there exist x belong to A, such that P(x)" in our language
      you see that (C or D) is an unnamed way to express alternative
      while "there exist x belong to A" is a named way to express alternative
      so, whenever we want to express alternative
      we would have to introduce name
      - we can add "or" back to our language later
        but I insist that we ignore "or" for now
        because I want the language to be simple
